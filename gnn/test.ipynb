{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "/home/reza/.local/lib/python3.10/site-packages/torch/lib/../../nvidia/cusparse/lib/libcusparse.so.12: undefined symbol: __nvJitLinkAddData_12_1, version libnvJitLink.so.12",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m prepare_data_vocab, live_feat\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m unpack_sequence\n",
      "File \u001b[0;32m~/Home/BU/HLS/GNN/gnnexamples/gnn/model.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch_geometric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GCNConv\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/__init__.py:237\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m USE_GLOBAL_DEPS:\n\u001b[1;32m    236\u001b[0m         _load_global_deps()\n\u001b[0;32m--> 237\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_C\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;66;03m# Appease the type checker; ordinarily this binding is inserted by the\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;66;03m# torch._C module initialization code in C\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n",
      "\u001b[0;31mImportError\u001b[0m: /home/reza/.local/lib/python3.10/site-packages/torch/lib/../../nvidia/cusparse/lib/libcusparse.so.12: undefined symbol: __nvJitLinkAddData_12_1, version libnvJitLink.so.12"
     ]
    }
   ],
   "source": [
    "from model import *\n",
    "from data import prepare_data_vocab, live_feat\n",
    "from torch.nn.utils.rnn import unpack_sequence\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch_geometric.data import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_idx, idx_to_word, data, data_emb = prepare_data_vocab(\"data\", func=live_feat, function_num=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(INFUNCTION_VAR * INFUNCTION_VAR)TMP_FUNCTION_NAME\t|(INFUNCTION_VAR *)TMP_VAR\t|*\t|+\t|-\t|-TMP_NUMBER\t|-TMP_VAR\t|/\t|;\t|INFUNCTION_VAR\t|INFUNCTION_VAR.INFUNCTION_VAR\t|TMP_FUNCTION_ARG_1\t|TMP_FUNCTION_ARG_10\t|TMP_FUNCTION_ARG_11\t|TMP_FUNCTION_ARG_2\t|TMP_FUNCTION_ARG_3\t|TMP_FUNCTION_ARG_4\t|TMP_FUNCTION_ARG_5\t|TMP_FUNCTION_ARG_6\t|TMP_FUNCTION_ARG_7\t|TMP_FUNCTION_ARG_8\t|TMP_FUNCTION_ARG_9\t|TMP_FUNCTION_NAME\t|TMP_NUMBER\t|TMP_POINTER_MEMBER\t|TMP_VAR\t|TMP_VAR(INFUNCTION_VAR)\t|TMP_VAR(TMP_FUNCTION_NAME)\t|assignment\t|char\t|double\t|float\t|function_call\t|gsl_error\t|int\t|load\t|math_op\t|phi\t|size_t\t|store\t|\n",
      "----------\n",
      " 40\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "for i in idx_to_word.keys():\n",
    "    print(idx_to_word[i], end='\\t|')\n",
    "    cnt+= 1\n",
    "\n",
    "print(\"\\n----------\\n\",cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define InputEncoder (RNN) and GNN\n",
    "in_dim = 8\n",
    "hidden_dim = 64\n",
    "num_layers = 7\n",
    "bidirectional = True\n",
    "out_dim    = 128\n",
    "\n",
    "int_dir = 2 if bidirectional else 1\n",
    "\n",
    "bb_enc = InputEncoder(word_to_idx, \n",
    "                      in_dim=in_dim, \n",
    "                      hidden_dim=hidden_dim, \n",
    "                      num_layers=num_layers, \n",
    "                      bidirectional=bidirectional)\n",
    " \n",
    "gnn = GNNModel(in_dim=int_dir*hidden_dim,\n",
    "               out_dim=out_dim,\n",
    "               layer_dims=[16, 32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dim = 8\n",
    "hidden_dim = 16\n",
    "num_layers = 7\n",
    "bidirectional = True\n",
    "out_dim    = 32\n",
    "\n",
    "int_dir = 2 if bidirectional else 1\n",
    "integrated = IntegratedModel(word_to_idx, \n",
    "                      in_dim_lstm=in_dim, \n",
    "                      hidden_dim=hidden_dim, \n",
    "                      num_layers=num_layers, \n",
    "                      bidirectional=bidirectional,\n",
    "                        out_dim=out_dim,\n",
    "                        layer_dims=[16, 32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(data)==len(data_emb)\n",
    "for i in range(len(data)):\n",
    "    assert len(data[i][2])==len(data_emb[i][1])\n",
    "\n",
    "#define InputEncoder (RNN) and GNN\n",
    "in_dim = 8\n",
    "hidden_dim = 16\n",
    "num_layers = 7\n",
    "bidirectional = True\n",
    "out_dim    = 16\n",
    "\n",
    "int_dir = 2 if bidirectional else 1\n",
    "\n",
    "bb_enc = InputEncoder(word_to_idx, \n",
    "                      in_dim=in_dim, \n",
    "                      hidden_dim=hidden_dim, \n",
    "                      num_layers=num_layers, \n",
    "                      bidirectional=bidirectional)\n",
    " \n",
    "gnn = GNNModel(in_dim=int_dir*hidden_dim,\n",
    "               out_dim=out_dim,\n",
    "               layer_dims=[16, 32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_node_embeddings = []\n",
    "new_data_emb = []\n",
    "for graph in data_emb:\n",
    "    edge_index, node_embs = graph\n",
    "    #if torch.tensor(edge_index).shape[0]==0:\n",
    "    if len(edge_index) != 0:\n",
    "        edge_index = torch.tensor(edge_index).T\n",
    "    else:\n",
    "        edge_index = torch.tensor([[0],[0]])\n",
    "    \n",
    "    out, h, c = bb_enc(node_embs)\n",
    "    node_feats = torch.vstack([k[-1] for k in unpack_sequence(out)])\n",
    "\n",
    "    assert node_feats.shape == (len(node_embs), int_dir*hidden_dim)\n",
    "\n",
    "            #Step 2: gnn forward pass\n",
    "    graph = Data(edge_index=edge_index,\n",
    "                        x=node_feats)\n",
    "    initial_node_embeddings.append(torch.rand((graph.num_nodes, out_dim), requires_grad=True))\n",
    "    new_data_emb.append([edge_index, node_embs, initial_node_embeddings])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index, node_embs = data_emb[0]\n",
    "#if torch.tensor(edge_index).shape[0]==0:\n",
    "\n",
    "if len(edge_index) != 0:\n",
    "    edge_index = torch.tensor(edge_index).T\n",
    "else:\n",
    "    edge_index = torch.tensor([[0],[0]])\n",
    "            \n",
    "        \n",
    "out, h, c = bb_enc(node_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For 5 functions: Epoch 1/1000, Average Loss: 30.03522\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [102], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m         total_items \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m bb\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     26\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 27\u001b[0m         \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Print or log the average loss for monitoring\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hidden_dim = 13\n",
    "ae = AE_rnn(word_to_idx, 7, hidden_dim, 3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(ae.parameters(), lr=0.01)\n",
    "epoch_num = 1000\n",
    "for epoch in range(epoch_num):\n",
    "    total_loss = 0\n",
    "    total_items = 0\n",
    "    j = -1\n",
    "    for graph in data_emb:\n",
    "        j += 1\n",
    "        edge_index, node_embs = graph\n",
    "        for bb in node_embs:\n",
    "\n",
    "            out, embs = ae(bb)\n",
    "            #print(a.shape)\n",
    "            #print(out)\n",
    "            #total_loss += unsupervised_loss.item()\n",
    "            #loss = criterion(out, embs)\n",
    "            loss = criterion(out, bb)\n",
    "            #loss = criterion(output, initial_node_embeddings[j])\n",
    "            # Backward pass and optimization\n",
    "            total_loss  += loss.item()\n",
    "            total_items += bb.shape[0]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    # Print or log the average loss for monitoring\n",
    "    if epoch%10==0 or epoch==epoch_num-1:\n",
    "        avg_loss = total_loss / len(data_emb)\n",
    "        print(f'For {j+1} functions: Epoch {epoch+1}/{epoch_num}, Average Loss: {avg_loss:.5f}', end='\\r')\n",
    "    if epoch%100==0 or epoch==epoch_num-1:\n",
    "        avg_loss = total_loss / len(data_emb)\n",
    "        print(f'For {j+1} functions: Epoch {epoch+1}/{epoch_num}, Average Loss: {avg_loss:.5f}', end='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 13\n",
    "ae = AE_gnnrnn(word_to_idx, 7, hidden_dim, 3)\n",
    "\n",
    "device = \"cuda\" \n",
    "ae_gpu  = ae.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected Tensor as element 0 in argument 0, but got tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [133], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data_emb_gpu \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_emb\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mTypeError\u001b[0m: expected Tensor as element 0 in argument 0, but got tuple"
     ]
    }
   ],
   "source": [
    "data_emb_gpu = torch.stack(data_emb).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For 578 functions: Epoch 1/1000, Average Loss: 16.72916\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [26], line 36\u001b[0m\n\u001b[1;32m     33\u001b[0m         total_items \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m bb\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     35\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 36\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Print or log the average loss for monitoring\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hidden_dim = 13\n",
    "ae = AE_gnnrnn(word_to_idx, 7, hidden_dim, 3)\n",
    "\n",
    "#device = \"cuda\" \n",
    "#ae = ae.cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(ae.parameters(), lr=0.01)\n",
    "epoch_num = 1000\n",
    "for epoch in range(epoch_num):\n",
    "    total_loss = 0\n",
    "    total_items = 0\n",
    "    j = -1\n",
    "    \n",
    "    for graph in data_emb:\n",
    "        j += 1\n",
    "        edge_index, node_embs = graph\n",
    "        #edge_index = edge_index.cuda()\n",
    "        #node_embs_gpu = [emb.cuda() for emb in node_embs]\n",
    "        #outs = ae(node_embs_gpu, edge_index)\n",
    "        outs = ae(node_embs, edge_index)\n",
    "        i = -1\n",
    "        for bb in node_embs:\n",
    "            i += 1\n",
    "            \n",
    "            loss = criterion(outs[i], bb)\n",
    "            #print(outs[i].shape)\n",
    "            #print(bb.shape)\n",
    "            #loss = criterion(output, initial_node_embeddings[j])\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            total_loss  += loss.item()\n",
    "            total_items += bb.shape[0]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # Print or log the average loss for monitoring\n",
    "    if epoch%10==0 or epoch==epoch_num-1:\n",
    "        avg_loss = total_loss / len(data_emb)\n",
    "        print(f'For {j+1} functions: Epoch {epoch+1}/{epoch_num}, Average Loss: {avg_loss:.5f}', end='\\r')\n",
    "    if epoch%100==0 or epoch==epoch_num-1:\n",
    "        avg_loss = total_loss / len(data_emb)\n",
    "        print(f'For {j+1} functions: Epoch {epoch+1}/{epoch_num}, Average Loss: {avg_loss:.5f}', end='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'batch_sizes'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [28], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m j \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     17\u001b[0m edge_index, node_embs \u001b[38;5;241m=\u001b[39m graph\n\u001b[0;32m---> 18\u001b[0m outs \u001b[38;5;241m=\u001b[39m \u001b[43mae\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode_embs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m bb \u001b[38;5;129;01min\u001b[39;00m node_embs:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Home/BU/HLS/GNN/gnnexamples/gnn/model.py:517\u001b[0m, in \u001b[0;36mAE_gnnrnn_gpu.forward\u001b[0;34m(self, x, edge_index)\u001b[0m\n\u001b[1;32m    515\u001b[0m packed_embs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39memb(packed_input\u001b[38;5;241m.\u001b[39mdata)\n\u001b[1;32m    516\u001b[0m packed_out, (h_n, c_n) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm(packed_embs)\n\u001b[0;32m--> 517\u001b[0m out, _ \u001b[38;5;241m=\u001b[39m \u001b[43mpad_packed_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpacked_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_first\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    518\u001b[0m last_hidden_states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([out[i, length \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, :] \u001b[38;5;28;01mfor\u001b[39;00m i, length \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(lengths)])\n\u001b[1;32m    519\u001b[0m state_h_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj1(last_hidden_states)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/utils/rnn.py:324\u001b[0m, in \u001b[0;36mpad_packed_sequence\u001b[0;34m(sequence, batch_first, padding_value, total_length)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpad_packed_sequence\u001b[39m(\n\u001b[1;32m    268\u001b[0m     sequence: PackedSequence,\n\u001b[1;32m    269\u001b[0m     batch_first: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    270\u001b[0m     padding_value: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m,\n\u001b[1;32m    271\u001b[0m     total_length: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    272\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Tensor, Tensor]:\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Pads a packed batch of variable length sequences.\u001b[39;00m\n\u001b[1;32m    274\u001b[0m \n\u001b[1;32m    275\u001b[0m \u001b[38;5;124;03m    It is an inverse operation to :func:`pack_padded_sequence`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    322\u001b[0m \n\u001b[1;32m    323\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 324\u001b[0m     max_seq_length \u001b[38;5;241m=\u001b[39m \u001b[43msequence\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_sizes\u001b[49m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m total_length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    326\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m total_length \u001b[38;5;241m<\u001b[39m max_seq_length:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'batch_sizes'"
     ]
    }
   ],
   "source": [
    "hidden_dim = 13\n",
    "ae = AE_gnnrnn_gpu(word_to_idx, 7, hidden_dim, 3)\n",
    "\n",
    "#device = \"cuda\" \n",
    "#ae.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(ae.parameters(), lr=0.01)\n",
    "epoch_num = 1000\n",
    "for epoch in range(epoch_num):\n",
    "    total_loss = 0\n",
    "    total_items = 0\n",
    "    j = -1\n",
    "    \n",
    "    for graph in data_emb:\n",
    "        j += 1\n",
    "        edge_index, node_embs = graph\n",
    "        outs = ae(node_embs, edge_index)\n",
    "        i = -1\n",
    "        for bb in node_embs:\n",
    "            i += 1\n",
    "            \n",
    "            loss = criterion(outs[i], bb)\n",
    "            #print(outs[i].shape)\n",
    "            #print(bb.shape)\n",
    "            #loss = criterion(output, initial_node_embeddings[j])\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            total_loss  += loss.item()\n",
    "            total_items += bb.shape[0]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # Print or log the average loss for monitoring\n",
    "    if epoch%10==0 or epoch==epoch_num-1:\n",
    "        avg_loss = total_loss / len(data_emb)\n",
    "        print(f'For {j+1} functions: Epoch {epoch+1}/{epoch_num}, Average Loss: {avg_loss:.5f}', end='\\r')\n",
    "    if epoch%100==0 or epoch==epoch_num-1:\n",
    "        avg_loss = total_loss / len(data_emb)\n",
    "        print(f'For {j+1} functions: Epoch {epoch+1}/{epoch_num}, Average Loss: {avg_loss:.5f}', end='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_target = torch.rand((len(data_emb), 1, out_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([578, 1, 32])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([ 8, 28, 25, 23,  8,  8,  8,  8,  8, 32,  9, 11, 14, 15,  8,  8, 36, 25,\n",
       "         25,  4, 23,  8,  8, 36, 25, 25,  7, 23,  8,  8,  8,  8,  8]),\n",
       " tensor([ 8, 36, 25, 25,  3, 23,  8,  8]),\n",
       " tensor([ 8, 37,  8,  8, 36, 25, 25,  3, 23,  8,  8, 36, 25, 25,  7, 23,  8,  8,\n",
       "          8,  8,  8]),\n",
       " tensor([ 8, 36, 25, 25,  3, 23,  8,  8]),\n",
       " tensor([ 8, 37,  8,  8, 36, 25, 25,  4, 23,  8,  8, 36, 25, 25,  7, 23,  8,  8,\n",
       "          8,  8,  8]),\n",
       " tensor([ 8, 36, 25, 25,  3, 23,  8,  8]),\n",
       " tensor([ 8, 37,  8, 32,  9, 11, 14, 15,  8,  8, 36, 25, 25,  2, 23,  8,  8,  8,\n",
       "         36, 25, 25,  2, 23,  8,  8,  8, 36, 25, 25,  2, 23,  8,  8,  8, 36, 25,\n",
       "         25,  4, 23,  8,  8,  8,  8,  8,  8]),\n",
       " tensor([ 8, 36, 25, 25,  3, 23,  8,  8]),\n",
       " tensor([ 8, 37,  8,  8, 36, 25, 25,  4, 23,  8,  8, 36, 25, 25,  7, 23,  8,  8,\n",
       "          8,  8,  8]),\n",
       " tensor([ 8, 36, 25, 25,  3, 23,  8,  8]),\n",
       " tensor([ 8, 37,  8,  8, 36, 25, 25,  4, 23,  8,  8, 36, 25, 25,  7, 23,  8,  8,\n",
       "          8,  8,  8]),\n",
       " tensor([ 8, 36, 25, 25,  3, 23,  8,  8]),\n",
       " tensor([ 8, 37,  8, 32,  9, 11,  8, 28, 25, 25,  8,  8,  8,  8]),\n",
       " tensor([8, 8, 8])]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_emb[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch_geometric.data import DataLoader as GraphDataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "class GraphDataset(Dataset):\n",
    "    def __init__(self, graphs):\n",
    "        self.graphs = graphs\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.graphs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.graphs[idx]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # Extract node features and edge indices from the batch\n",
    "    \n",
    "    node_features = [graph[1] for graph in batch]\n",
    "    edge_indices = [graph[0] for graph in batch]\n",
    "    \n",
    "    # Pad sequences to make them the same size within a batch\n",
    "    node_features_padded = pad_sequence(node_features, batch_first=True, padding_value=0)\n",
    "    edge_indices_padded = pad_sequence(edge_indices, batch_first=True, padding_value=-1)\n",
    "    \n",
    "    return node_features_padded, edge_indices_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/reza/.local/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/reza/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n  File \"/home/reza/.local/lib/python3.10/site-packages/torch_geometric/loader/dataloader.py\", line 35, in __call__\n    return [self(s) for s in zip(*batch)]\n  File \"/home/reza/.local/lib/python3.10/site-packages/torch_geometric/loader/dataloader.py\", line 35, in <listcomp>\n    return [self(s) for s in zip(*batch)]\n  File \"/home/reza/.local/lib/python3.10/site-packages/torch_geometric/loader/dataloader.py\", line 23, in __call__\n    return default_collate(batch)\n  File \"/home/reza/.local/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 265, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n  File \"/home/reza/.local/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 119, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n  File \"/home/reza/.local/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 161, in collate_tensor_fn\n    out = elem.new(storage).resize_(len(batch), *list(elem.size()))\nRuntimeError: Trying to resize storage that is not resizable\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [41], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m num_workers \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m\n\u001b[1;32m      5\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m GraphDataLoader(data_emb, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39mshuffle, num_workers\u001b[38;5;241m=\u001b[39mnum_workers)\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m      7\u001b[0m         node_features_batch, edge_indices_batch \u001b[38;5;241m=\u001b[39m batch\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1345\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1343\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1344\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1371\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1369\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1371\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1372\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_utils.py:644\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    641\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    642\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    643\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m--> 644\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/reza/.local/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/reza/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n  File \"/home/reza/.local/lib/python3.10/site-packages/torch_geometric/loader/dataloader.py\", line 35, in __call__\n    return [self(s) for s in zip(*batch)]\n  File \"/home/reza/.local/lib/python3.10/site-packages/torch_geometric/loader/dataloader.py\", line 35, in <listcomp>\n    return [self(s) for s in zip(*batch)]\n  File \"/home/reza/.local/lib/python3.10/site-packages/torch_geometric/loader/dataloader.py\", line 23, in __call__\n    return default_collate(batch)\n  File \"/home/reza/.local/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 265, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n  File \"/home/reza/.local/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 119, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n  File \"/home/reza/.local/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 161, in collate_tensor_fn\n    out = elem.new(storage).resize_(len(batch), *list(elem.size()))\nRuntimeError: Trying to resize storage that is not resizable\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.data import DataLoader as GraphDataLoader\n",
    "batch_size = 64\n",
    "shuffle = True\n",
    "num_workers = 4\n",
    "train_loader = GraphDataLoader(data_emb, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers)\n",
    "for batch in train_loader:\n",
    "        node_features_batch, edge_indices_batch = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GraphDataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [44], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m shuffle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m      3\u001b[0m num_workers \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m\n\u001b[0;32m----> 4\u001b[0m dataset    \u001b[38;5;241m=\u001b[39m \u001b[43mGraphDataset\u001b[49m(data_emb)\n\u001b[1;32m      5\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39mshuffle, collate_fn\u001b[38;5;241m=\u001b[39mcollate_fn)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m dataloader:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'GraphDataset' is not defined"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "shuffle = True\n",
    "num_workers = 4\n",
    "dataset    = GraphDataset(data_emb)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, collate_fn=collate_fn)\n",
    "for batch in dataloader:\n",
    "        node_features_batch, edge_indices_batch = batch\n",
    "        print(node_features_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0322,  0.0419,  0.1197,  0.1828,  0.0559,  0.0648, -0.0141,  0.0522],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor([-0.3529,  0.2771,  0.2931,  1.6526, -1.7671, -0.0654, -1.2059, -1.6384],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor([ 0.0322,  0.0419,  0.1197,  0.1828,  0.0559,  0.0648, -0.0141,  0.0522],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor([-0.3529,  0.2771,  0.2931,  1.6526, -1.7671, -0.0654, -1.2059, -1.6384],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [16, 8]] is at version 2; expected version 1 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [5], line 33\u001b[0m\n\u001b[1;32m     31\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     32\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output[i][j]\u001b[38;5;241m.\u001b[39mT[\u001b[38;5;241m0\u001b[39m], emb[i][j])  \u001b[38;5;66;03m# Compare with original sequence\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [16, 8]] is at version 2; expected version 1 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!"
     ]
    }
   ],
   "source": [
    "in_dim = 8\n",
    "hidden_dim = 16\n",
    "num_layers = 1\n",
    "bidirectional = True\n",
    "out_dim    = 4\n",
    "\n",
    "int_dir = 2 if bidirectional else 1\n",
    "out_dim    = int(in_dim/int_dir)\n",
    "\n",
    "model = AutoEncoder(word_to_idx, \n",
    "                    in_dim=in_dim, \n",
    "                    hidden_dim=hidden_dim, \n",
    "                    num_layers=num_layers, \n",
    "                    output_size=out_dim,\n",
    "                    bidirectional=bidirectional)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "criterion = nn.MSELoss()\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for graph in data_emb:\n",
    "        edge_index, node_embs = graph\n",
    "        output, emb = model(node_embs)\n",
    "        \n",
    "        for i in range(len(emb)):\n",
    "            for j in range(len(emb[i])):\n",
    "                print(output[i][j].T[0])\n",
    "                print(emb[i][j])\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss = criterion(output[i][j].T[0], emb[i][j])  # Compare with original sequence\n",
    "                loss.backward()\n",
    "                optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min: -0.2046039701, Mean: -0.0094097275, Max: 0.1965838373 |#| For 5 functions: Epoch 1/100, Average Loss: 0.0005420736\n",
      "Min: -0.0348513164, Mean: -0.0025811526, Max: 0.0398739651 |#| For 2 functions: Epoch 3/100, Average Loss: 0.0003829842\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [16], line 39\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m#if torch.tensor(edge_index).shape[0]==0:\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m#random_indices = random.sample(range(0, 128), 8)\u001b[39;00m\n\u001b[1;32m     36\u001b[0m random_indices \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m10\u001b[39m\u001b[38;5;241m*\u001b[39mx \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m((\u001b[38;5;28mint\u001b[39m)(int_dir\u001b[38;5;241m*\u001b[39mhidden_dim\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m10\u001b[39m))]        \n\u001b[0;32m---> 39\u001b[0m gnn_output, node_feats \u001b[38;5;241m=\u001b[39m \u001b[43mintegrated\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode_embs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m#gnn_output, node_feats = integrated(node_features_batch, edge_indices_batch)\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m node_feats\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m (\u001b[38;5;28mlen\u001b[39m(node_embs), int_dir\u001b[38;5;241m*\u001b[39mhidden_dim)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Home/BU/HLS/GNN/gnnexamples/gnn/model.py:177\u001b[0m, in \u001b[0;36mIntegratedModel.forward\u001b[0;34m(self, node_sequences, edge_index)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, node_sequences, edge_index):\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;66;03m# node_sequences: shape (num_nodes, seq_length, lstm_input_size)\u001b[39;00m\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;66;03m# edge_index: shape (2, num_edges)\u001b[39;00m\n\u001b[0;32m--> 177\u001b[0m     out, h, c \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode_sequences\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m     node_feats \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mvstack([k[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m unpack_sequence(out)])\n\u001b[1;32m    179\u001b[0m     original_array \u001b[38;5;241m=\u001b[39m node_feats\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Home/BU/HLS/GNN/gnnexamples/gnn/model.py:88\u001b[0m, in \u001b[0;36mInputEncoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;66;03m#x should be per-node bb ins indices\u001b[39;00m\n\u001b[1;32m     87\u001b[0m     embs \u001b[38;5;241m=\u001b[39m pack_sequence([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleaky_relu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39memb(bb)) \u001b[38;5;28;01mfor\u001b[39;00m bb \u001b[38;5;129;01min\u001b[39;00m x], enforce_sorted\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 88\u001b[0m     out, (h_n, c_n) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43membs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;66;03m#out = self.leaky_relu(out) # Apply Leaky ReLU\u001b[39;00m\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out, h_n, c_n\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/rnn.py:815\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    812\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers,\n\u001b[1;32m    813\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first)\n\u001b[1;32m    814\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 815\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    816\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    817\u001b[0m output \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    818\u001b[0m hidden \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;241m1\u001b[39m:]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "https://github.com/TreeinRandomForest/gnnexamples.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0126,  0.0034,  0.1862,  0.0372,  0.0126,  0.1426, -0.0502,  0.0251,\n",
       "          0.0074, -0.0424,  0.0644, -0.0057,  0.1713, -0.2475, -0.2109,  0.2017,\n",
       "          0.0335,  0.0390, -0.0477, -0.0328,  0.0037,  0.0888,  0.0216, -0.0074,\n",
       "          0.0678,  0.0013,  0.0570, -0.0472, -0.0675,  0.0315,  0.0022, -0.0574],\n",
       "        [-0.0119,  0.0015,  0.1829,  0.0377,  0.0145,  0.1457, -0.0493,  0.0237,\n",
       "          0.0046, -0.0451,  0.0696, -0.0094,  0.1666, -0.2515, -0.2126,  0.2000,\n",
       "          0.0315,  0.0395, -0.0445, -0.0329,  0.0015,  0.0881,  0.0178, -0.0092,\n",
       "          0.0670,  0.0021,  0.0529, -0.0489, -0.0652,  0.0246,  0.0071, -0.0584]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-7.3550e-03, -2.7184e-03,  1.9277e-01,  3.8857e-02, -2.6345e-03,\n",
       "          1.4869e-01, -5.2959e-02,  2.5981e-02,  5.1645e-03, -3.0783e-02,\n",
       "          6.8217e-02, -1.3734e-02,  1.8695e-01, -2.6015e-01, -2.3863e-01,\n",
       "          2.4312e-01,  3.2007e-02,  4.0228e-02, -4.7154e-02, -3.0721e-02,\n",
       "          2.4937e-03,  8.9804e-02,  1.9781e-02, -6.0409e-03,  6.5731e-02,\n",
       "          4.7992e-03,  5.6716e-02, -4.7487e-02, -6.8936e-02,  3.0280e-02,\n",
       "          4.6410e-03, -5.6473e-02],\n",
       "        [-1.0848e-02,  8.0277e-03,  1.6795e-01,  3.6411e-02,  3.1916e-02,\n",
       "          1.3602e-01, -5.0221e-02,  2.0493e-02,  5.5699e-03, -5.6578e-02,\n",
       "          6.6495e-02, -2.0384e-03,  1.5436e-01, -2.4501e-01, -1.9379e-01,\n",
       "          1.5640e-01,  3.2156e-02,  4.0920e-02, -4.6510e-02, -2.9881e-02,\n",
       "         -1.8220e-04,  8.8291e-02,  1.9627e-02, -9.0167e-03,  6.8013e-02,\n",
       "         -6.0287e-04,  5.4320e-02, -4.9530e-02, -6.3678e-02,  2.4718e-02,\n",
       "          3.4426e-03, -6.2146e-02]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max:  128\n",
      "Min:  2\n"
     ]
    }
   ],
   "source": [
    "min = 10000\n",
    "max = 0\n",
    "for graph in data_emb:\n",
    "    edge_index, node_embs = graph\n",
    "    if len(edge_index) != 0:\n",
    "        if len(node_embs)>max:\n",
    "            max = len(node_embs)\n",
    "        if len(node_embs)<min:\n",
    "            min = len(node_embs)\n",
    "print(\"Max: \", max)\n",
    "print(\"Min: \", min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gnn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [21], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss()\n\u001b[0;32m----> 2\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(\u001b[43mgnn\u001b[49m\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m      4\u001b[0m epoch_num \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m300\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Training loop\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gnn' is not defined"
     ]
    }
   ],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(gnn.parameters(), lr=0.01)\n",
    "\n",
    "epoch_num = 300\n",
    "# Training loop\n",
    "\n",
    "random_target = torch.rand((len(data_emb), 1, out_dim))\n",
    "stop = 0\n",
    "for epoch in range(epoch_num):\n",
    "    total_loss = 0\n",
    "    total_items = 0\n",
    "    j = -1\n",
    "    for graph in data_emb:\n",
    "        j += 1\n",
    "        edge_index, node_embs = graph\n",
    "        #if torch.tensor(edge_index).shape[0]==0:\n",
    "        if len(edge_index) != 0:\n",
    "            edge_index = torch.tensor(edge_index).T\n",
    "        else:\n",
    "            edge_index = torch.tensor([[0],[0]])\n",
    "            \n",
    "        \n",
    "        out, h, c = bb_enc(node_embs)\n",
    "        node_feats = torch.vstack([k[-1] for k in unpack_sequence(out)])\n",
    "\n",
    "        assert node_feats.shape == (len(node_embs), int_dir*hidden_dim)\n",
    "\n",
    "                #Step 2: gnn forward pass\n",
    "        graph = Data(edge_index=edge_index,\n",
    "                            x=node_feats)\n",
    "        \n",
    "        output = gnn(graph)\n",
    "        \n",
    "        # Unsupervised loss (dummy loss for illustration)\n",
    "        loss = criterion(output, random_target[j])\n",
    "        #loss = criterion(output, initial_node_embeddings[j])\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        total_loss  += loss.item()\n",
    "        total_items += graph.x.shape[0]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch%10==0 or epoch==epoch_num-1:\n",
    "            avg_loss = total_loss / len(data_emb)\n",
    "            print(f'For {j+1} functions: Epoch {epoch+1}/{epoch_num}, Average Loss: {avg_loss:.5f}', end='\\r')\n",
    "\n",
    "        #total_loss += unsupervised_loss.item()\n",
    "\n",
    "    # Print or log the average loss for monitoring\n",
    "    if epoch%10==0 or epoch==epoch_num-1:\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.6942e-06,  3.6233e-06, -7.3788e-04, -4.4494e-05,  3.6724e-08,\n",
       "         -7.6004e-01, -2.7473e-03,  5.2556e-04,  3.6020e-06, -4.5144e-05,\n",
       "         -9.2002e-07,  6.8339e-02, -2.3965e-04,  1.6933e-03, -1.9910e-04,\n",
       "         -2.3565e-04,  4.8033e-02,  9.4108e-02, -7.9327e-02,  1.0815e-01,\n",
       "          1.6961e-02, -1.9245e-02, -3.6383e-03,  1.5153e-01, -4.3944e-02,\n",
       "         -2.2803e-02, -7.2129e-02,  7.6061e-02, -3.1655e-02, -1.4341e-01,\n",
       "          9.2256e-02, -1.8506e-01],\n",
       "        [-1.6942e-06,  3.6233e-06, -7.3788e-04, -4.4494e-05,  3.6724e-08,\n",
       "         -7.6004e-01, -2.7473e-03,  5.2556e-04,  3.6020e-06, -4.5144e-05,\n",
       "         -9.2002e-07,  6.8339e-02, -2.3965e-04,  1.6933e-03, -1.9910e-04,\n",
       "         -2.3565e-04,  4.8033e-02,  9.4108e-02, -7.9327e-02,  1.0815e-01,\n",
       "          1.6961e-02, -1.9245e-02, -3.6383e-03,  1.5153e-01, -4.3944e-02,\n",
       "         -2.2803e-02, -7.2129e-02,  7.6061e-02, -3.1655e-02, -1.4341e-01,\n",
       "          9.2256e-02, -1.8506e-01],\n",
       "        [-1.6942e-06,  3.6233e-06, -7.3788e-04, -4.4494e-05,  3.6724e-08,\n",
       "         -7.6004e-01, -2.7473e-03,  5.2556e-04,  3.6020e-06, -4.5144e-05,\n",
       "         -9.2002e-07,  6.8339e-02, -2.3965e-04,  1.6933e-03, -1.9910e-04,\n",
       "         -2.3565e-04,  4.8033e-02,  9.4108e-02, -7.9327e-02,  1.0815e-01,\n",
       "          1.6961e-02, -1.9245e-02, -3.6383e-03,  1.5153e-01, -4.3944e-02,\n",
       "         -2.2803e-02, -7.2129e-02,  7.6061e-02, -3.1655e-02, -1.4341e-01,\n",
       "          9.2256e-02, -1.8506e-01],\n",
       "        [-1.1700e-06,  7.5764e-07, -1.3019e-04, -1.6162e-05,  1.2953e-08,\n",
       "         -9.9966e-01, -5.8828e-03,  7.0171e-05,  7.6033e-07, -1.3339e-05,\n",
       "         -1.7024e-07,  1.5966e-01, -1.4535e-04,  3.6517e-04, -2.3754e-05,\n",
       "         -5.3328e-05,  5.4880e-02,  8.5114e-02, -7.9509e-02,  1.1086e-01,\n",
       "          3.5674e-02, -3.6125e-02, -9.2807e-03,  1.8709e-01, -3.2892e-02,\n",
       "         -3.5655e-02, -5.6451e-02,  6.7804e-02, -1.4693e-02, -1.3732e-01,\n",
       "          1.0297e-01, -1.9425e-01],\n",
       "        [-1.6942e-06,  3.6233e-06, -7.3788e-04, -4.4494e-05,  3.6724e-08,\n",
       "         -7.6004e-01, -2.7473e-03,  5.2556e-04,  3.6020e-06, -4.5144e-05,\n",
       "         -9.2002e-07,  6.8339e-02, -2.3965e-04,  1.6933e-03, -1.9910e-04,\n",
       "         -2.3565e-04,  4.8033e-02,  9.4108e-02, -7.9327e-02,  1.0815e-01,\n",
       "          1.6961e-02, -1.9245e-02, -3.6383e-03,  1.5153e-01, -4.3944e-02,\n",
       "         -2.2803e-02, -7.2129e-02,  7.6061e-02, -3.1655e-02, -1.4341e-01,\n",
       "          9.2256e-02, -1.8506e-01]])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lstm_encoder.emb.weight grad is None\n",
      "lstm_encoder.lstm.weight_ih_l0 grad is None\n",
      "lstm_encoder.lstm.weight_hh_l0 grad is None\n",
      "lstm_encoder.lstm.bias_ih_l0 grad is None\n",
      "lstm_encoder.lstm.bias_hh_l0 grad is None\n",
      "lstm_encoder.lstm.weight_ih_l0_reverse grad is None\n",
      "lstm_encoder.lstm.weight_hh_l0_reverse grad is None\n",
      "lstm_encoder.lstm.bias_ih_l0_reverse grad is None\n",
      "lstm_encoder.lstm.bias_hh_l0_reverse grad is None\n",
      "lstm_encoder.lstm.weight_ih_l1 grad is None\n",
      "lstm_encoder.lstm.weight_hh_l1 grad is None\n",
      "lstm_encoder.lstm.bias_ih_l1 grad is None\n",
      "lstm_encoder.lstm.bias_hh_l1 grad is None\n",
      "lstm_encoder.lstm.weight_ih_l1_reverse grad is None\n",
      "lstm_encoder.lstm.weight_hh_l1_reverse grad is None\n",
      "lstm_encoder.lstm.bias_ih_l1_reverse grad is None\n",
      "lstm_encoder.lstm.bias_hh_l1_reverse grad is None\n",
      "lstm_encoder.lstm.weight_ih_l2 grad is None\n",
      "lstm_encoder.lstm.weight_hh_l2 grad is None\n",
      "lstm_encoder.lstm.bias_ih_l2 grad is None\n",
      "lstm_encoder.lstm.bias_hh_l2 grad is None\n",
      "lstm_encoder.lstm.weight_ih_l2_reverse grad is None\n",
      "lstm_encoder.lstm.weight_hh_l2_reverse grad is None\n",
      "lstm_encoder.lstm.bias_ih_l2_reverse grad is None\n",
      "lstm_encoder.lstm.bias_hh_l2_reverse grad is None\n",
      "lstm_encoder.lstm.weight_ih_l3 grad is None\n",
      "lstm_encoder.lstm.weight_hh_l3 grad is None\n",
      "lstm_encoder.lstm.bias_ih_l3 grad is None\n",
      "lstm_encoder.lstm.bias_hh_l3 grad is None\n",
      "lstm_encoder.lstm.weight_ih_l3_reverse grad is None\n",
      "lstm_encoder.lstm.weight_hh_l3_reverse grad is None\n",
      "lstm_encoder.lstm.bias_ih_l3_reverse grad is None\n",
      "lstm_encoder.lstm.bias_hh_l3_reverse grad is None\n",
      "lstm_encoder.lstm.weight_ih_l4 grad is None\n",
      "lstm_encoder.lstm.weight_hh_l4 grad is None\n",
      "lstm_encoder.lstm.bias_ih_l4 grad is None\n",
      "lstm_encoder.lstm.bias_hh_l4 grad is None\n",
      "lstm_encoder.lstm.weight_ih_l4_reverse grad is None\n",
      "lstm_encoder.lstm.weight_hh_l4_reverse grad is None\n",
      "lstm_encoder.lstm.bias_ih_l4_reverse grad is None\n",
      "lstm_encoder.lstm.bias_hh_l4_reverse grad is None\n",
      "lstm_encoder.lstm.weight_ih_l5 grad is None\n",
      "lstm_encoder.lstm.weight_hh_l5 grad is None\n",
      "lstm_encoder.lstm.bias_ih_l5 grad is None\n",
      "lstm_encoder.lstm.bias_hh_l5 grad is None\n",
      "lstm_encoder.lstm.weight_ih_l5_reverse grad is None\n",
      "lstm_encoder.lstm.weight_hh_l5_reverse grad is None\n",
      "lstm_encoder.lstm.bias_ih_l5_reverse grad is None\n",
      "lstm_encoder.lstm.bias_hh_l5_reverse grad is None\n",
      "lstm_encoder.lstm.weight_ih_l6 grad is None\n",
      "lstm_encoder.lstm.weight_hh_l6 grad is None\n",
      "lstm_encoder.lstm.bias_ih_l6 grad is None\n",
      "lstm_encoder.lstm.bias_hh_l6 grad is None\n",
      "lstm_encoder.lstm.weight_ih_l6_reverse grad is None\n",
      "lstm_encoder.lstm.weight_hh_l6_reverse grad is None\n",
      "lstm_encoder.lstm.bias_ih_l6_reverse grad is None\n",
      "lstm_encoder.lstm.bias_hh_l6_reverse grad is None\n",
      "gnn_model.conv_layers.0.bias grad is None\n",
      "gnn_model.conv_layers.0.lin.weight grad is None\n",
      "gnn_model.conv_layers.1.bias grad is None\n",
      "gnn_model.conv_layers.1.lin.weight grad is None\n",
      "gnn_model.fc.weight grad is None\n",
      "gnn_model.fc.bias grad is None\n"
     ]
    }
   ],
   "source": [
    "for name, param in integrated.named_parameters():\n",
    "    if param.grad is not None:\n",
    "        print(f\"{name} grad norm: {param.grad.norm()}\")\n",
    "    else:\n",
    "        print(f\"{name} grad is None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emb.weight grad norm: 4.2484352889005095e-05\n",
      "lstm.weight_ih_l0 grad norm: 0.0003254949115216732\n",
      "lstm.weight_hh_l0 grad norm: 7.605506834806874e-05\n",
      "lstm.bias_ih_l0 grad norm: 0.0003012344823218882\n",
      "lstm.bias_hh_l0 grad norm: 0.0003012344823218882\n",
      "lstm.weight_ih_l0_reverse grad norm: 0.00032709658262319863\n",
      "lstm.weight_hh_l0_reverse grad norm: 7.603189442306757e-05\n",
      "lstm.bias_ih_l0_reverse grad norm: 0.000313809810904786\n",
      "lstm.bias_hh_l0_reverse grad norm: 0.000313809810904786\n",
      "lstm.weight_ih_l1 grad norm: 0.000558680621907115\n",
      "lstm.weight_hh_l1 grad norm: 0.00022624272969551384\n",
      "lstm.bias_ih_l1 grad norm: 0.0010487716645002365\n",
      "lstm.bias_hh_l1 grad norm: 0.0010487715480849147\n",
      "lstm.weight_ih_l1_reverse grad norm: 0.0004790965758729726\n",
      "lstm.weight_hh_l1_reverse grad norm: 0.00016279509873129427\n",
      "lstm.bias_ih_l1_reverse grad norm: 0.0009165560477413237\n",
      "lstm.bias_hh_l1_reverse grad norm: 0.0009165561059489846\n",
      "lstm.weight_ih_l2 grad norm: 0.0014243311015889049\n",
      "lstm.weight_hh_l2 grad norm: 0.0006491430685855448\n",
      "lstm.bias_ih_l2 grad norm: 0.0031884764321148396\n",
      "lstm.bias_hh_l2 grad norm: 0.003188476664945483\n",
      "lstm.weight_ih_l2_reverse grad norm: 0.0012144839856773615\n",
      "lstm.weight_hh_l2_reverse grad norm: 0.000511831371113658\n",
      "lstm.bias_ih_l2_reverse grad norm: 0.0027420001570135355\n",
      "lstm.bias_hh_l2_reverse grad norm: 0.0027420001570135355\n",
      "lstm.weight_ih_l3 grad norm: 0.005598713643848896\n",
      "lstm.weight_hh_l3 grad norm: 0.002777466317638755\n",
      "lstm.bias_ih_l3 grad norm: 0.012291979975998402\n",
      "lstm.bias_hh_l3 grad norm: 0.012291979975998402\n",
      "lstm.weight_ih_l3_reverse grad norm: 0.0041890740394592285\n",
      "lstm.weight_hh_l3_reverse grad norm: 0.0012915530242025852\n",
      "lstm.bias_ih_l3_reverse grad norm: 0.009323948994278908\n",
      "lstm.bias_hh_l3_reverse grad norm: 0.009323948062956333\n",
      "lstm.weight_ih_l4 grad norm: 0.014585752040147781\n",
      "lstm.weight_hh_l4 grad norm: 0.00749950809404254\n",
      "lstm.bias_ih_l4 grad norm: 0.03295409306883812\n",
      "lstm.bias_hh_l4 grad norm: 0.03295409306883812\n",
      "lstm.weight_ih_l4_reverse grad norm: 0.014980059117078781\n",
      "lstm.weight_hh_l4_reverse grad norm: 0.004400774370878935\n",
      "lstm.bias_ih_l4_reverse grad norm: 0.03397384658455849\n",
      "lstm.bias_hh_l4_reverse grad norm: 0.03397384658455849\n",
      "lstm.weight_ih_l5 grad norm: 0.05398128181695938\n",
      "lstm.weight_hh_l5 grad norm: 0.026839403435587883\n",
      "lstm.bias_ih_l5 grad norm: 0.11788318306207657\n",
      "lstm.bias_hh_l5 grad norm: 0.11788317561149597\n",
      "lstm.weight_ih_l5_reverse grad norm: 0.04543125629425049\n",
      "lstm.weight_hh_l5_reverse grad norm: 0.008586478419601917\n",
      "lstm.bias_ih_l5_reverse grad norm: 0.1012001782655716\n",
      "lstm.bias_hh_l5_reverse grad norm: 0.1012001782655716\n",
      "lstm.weight_ih_l6 grad norm: 0.21125857532024384\n",
      "lstm.weight_hh_l6 grad norm: 0.12833844125270844\n",
      "lstm.bias_ih_l6 grad norm: 0.47973892092704773\n",
      "lstm.bias_hh_l6 grad norm: 0.47973892092704773\n",
      "lstm.weight_ih_l6_reverse grad norm: 0.12436791509389877\n",
      "lstm.weight_hh_l6_reverse grad norm: 0.0\n",
      "lstm.bias_ih_l6_reverse grad norm: 0.30265215039253235\n",
      "lstm.bias_hh_l6_reverse grad norm: 0.30265215039253235\n"
     ]
    }
   ],
   "source": [
    "for name, param in bb_enc.named_parameters():\n",
    "    if param.grad is not None:\n",
    "        print(f\"{name} grad norm: {param.grad.norm()}\")\n",
    "    else:\n",
    "        print(f\"{name} grad is None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in integrated.named_parameters():\n",
    "    if not param.requires_grad:\n",
    "        print(\"not\", param)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'generator' object has no attribute 'grad'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'generator' object has no attribute 'grad'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: _1\n",
      "op: *\n",
      "b: 5.0e-1\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def extract_a_op_b(input_string):\n",
    "    \n",
    "\n",
    "    # Find and replace expressions within parentheses with a placeholder\n",
    "    placeholder = '__PLACEHOLDER__'\n",
    "    flag_par = 0\n",
    "    while '(' in input_string and ')' in input_string:\n",
    "        flag_par = 1\n",
    "        # Find the innermost expression within parentheses\n",
    "        inner_expr = re.search(r'\\([^)]*\\)', input_string).group(0)\n",
    "        \n",
    "        # Replace the inner expression with a placeholder\n",
    "        input_string = input_string.replace(inner_expr, placeholder)\n",
    "\n",
    "    # Define the original pattern\n",
    "    original_pattern = r\"([-+]?\\b\\d+(\\.\\d*)?(e[+-]?\\d+)?\\b|\\b\\w+\\b|\\[\\s*\\d+(\\s*,\\s*\\d+)*\\s*\\])\\s*([+\\-*/])\\s*([-+]?\\b\\d+(\\.\\d*)?(e[+-]?\\d+)?\\b|\\b\\w+\\b|\\[\\s*\\d+(\\s*,\\s*\\d+)*\\s*\\])\"\n",
    "\n",
    "    # Find the first match in the modified input string\n",
    "    match = re.search(original_pattern, input_string)\n",
    "\n",
    "    if match:\n",
    "        # Extract a, op, b from the match\n",
    "        a = match.group(1)\n",
    "        op = match.group(5)\n",
    "        b = match.group(6)\n",
    "\n",
    "        # Replace the placeholder back with the original expression\n",
    "        if flag_par:\n",
    "            a = a.replace(placeholder, inner_expr)\n",
    "            b = b.replace(placeholder, inner_expr)\n",
    "\n",
    "        return a, op, b\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "input_string = \"_1 * 5.0e-1\"\n",
    "result = extract_a_op_b(input_string)\n",
    "\n",
    "if result:\n",
    "    a, op, b = result\n",
    "    print(\"a:\", a)\n",
    "    print(\"op:\", op)\n",
    "    print(\"b:\", b)\n",
    "else:\n",
    "    print(\"No match found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "vocab = np.array(['a', 'b', 'c', 'a'])\n",
    "vocab = np.unique(vocab)\n",
    "word_to_idx = dict(zip(vocab, np.arange(len(vocab))))\n",
    "idx_to_word = dict(zip(np.arange(len(vocab)), vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 0, 'b': 1, 'c': 2}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
